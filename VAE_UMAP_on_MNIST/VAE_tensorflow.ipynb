{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b14df16e-0a02-49a8-a477-7cd84f80fc0e","_uuid":"5bff66d63dca5b4b08f9c69e554a6177895dc670"},"source":["# Visualizing MNIST with a Deep Variational Autoencoder\n","\n","1. Introduction  \n","    A. What is autoencoding?  \n","    B. Autoencoders  \n","    C. The Variational Variety  \n","2. Data preparation  \n","    A. Load data   \n","    B. Reshape & normalize  \n","3. Model construction  \n","    A. Encoder network  \n","    B. Sampling function  \n","    C. Decoder network  \n","    D. Loss\n","4. Train the VAE\n","5. Results  \n","    A. Clustering of digits in the latent space  \n","    B. Sample digits"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9232ab89-1026-4c9e-93d1-dd9a7e3f8f8f","_uuid":"9184c1178ebfec6552bcc5ced94b98a22df9a24b"},"source":["## *1. Introduction*\n","Variational Autoencoders (VAEs) can be used to visualize high-dimensional data in a meaningful, lower-dimensional space. In this kernel, I go over some details about autoencoding and autoencoders, especially VAEs, before constructing and training a deep VAE on the MNIST data from the Digit Recognizer competition. We'll see how the data cluster in the lower-dimensional space according to their digit class. Plotting the test set data in this space shows where the images with unknown digit classes fall with respect to the known digit classes.\n","\n","The code here borrows heavily from Fran&ccedil;ois Chollet's example VAE from his book [Deep Learning with Python](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_1?ie=UTF8&qid=1520470984&sr=8-1&keywords=francois+chollet). You can find a repo of examples from the book (including the one that inspired this kernel) [here on GitHub](https://github.com/brilliantFire/deep-learning-with-python-notebooks).\n","\n","### *A. What is autoencoding?*\n","Autoencoding is much like what it sounds in the sense that the input and 'output' are essentially the same. It's an algorithm for data compression where the functions for compression and decompression are *learned from the data*. It's considered more of a *semi-supervised* learning method as opposed to a truly *unsupervised* one since it's not entirely 'targetless'. Instead it learns the targets from the data itself.\n","\n","Despite all this talk of data compression, autoencoders aren't typically used for that purpose. In practice, you're much more likely to see them being used to preprocess data (as in denoising - think images but it doesn't have to be ;) ) or for dimensionality reduction. In fact, the hidden layers of simple autoencoders are doing something like principal component analysis (PCA), another method traditionally used for dimensionality reduction.\n","\n","### *B. Autoencoders*\n","Generally autoencoders have three parts: an encoder, a decoder, and a 'loss' function that maps one to the other. For the simplest autoencoders - the sort that compress and then reconstruct the original inputs from the compressed representation - we can think of the 'loss' as describing the amount of information lost in the process of reconstruction. Typically when people are talking about autoencoders, they're talking about ones where the encoders and decoders are neural networks (in our case deep convnets). In training the autoencoder, we're optimizing the parameters of the neural networks to minimize the 'loss' (or distance) and we do that by stochastic gradient descent (yet another topic for another post). \n","\n","### *C. The Variational Variety*\n","There's a bunch of different kinds of autoencoders but for this post I'm going to concentrate on one type called a *variational autoencoder*. Variational autoencoders (VAEs) don't learn to morph the data in and out of a compressed representation of itself like the 'vanilla' autoencoders I described above. Instead, they learn the parameters of the probability distribution that the data came from. These types of autoencoders have much in common with latent factor analysis (if you know something about that). The encoder and decoder learn models that are in terms of underlying, unobserved *latent* variables. It's essentially an inference model and a generative model daisy-chained together.\n","\n","![A variational autoencoder](https://i.imgur.com/ZN6MyTx.png)\n","\n","VAEs have received a lot of attention because of their *generative* ability (though they seem to be falling out of fashion in favor of general adversarial networks, or GANs, in that regard). Since they learn about the distribution the inputs came from, we can sample from that distribution to generate novel data. As we'll see, VAEs can also be used to cluster data in useful ways."]},{"cell_type":"markdown","metadata":{"_cell_guid":"af039539-b12e-4573-a78f-b550f44c7332","_uuid":"517052cecc729f77b352d0d6e74f35e8489aa474"},"source":["## 2. *Data preparation*  \n","### *A. Load Data*"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-19 11:08:37.043827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-05-19 11:08:37.044193: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"ename":"AttributeError","evalue":"module 'keras.engine.base_layer' has no attribute 'BaseRandomLayer'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/VAE.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/VAE.ipynb#ch0000003vscode-remote?line=5'>6</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/VAE.ipynb#ch0000003vscode-remote?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/VAE.ipynb#ch0000003vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/VAE.ipynb#ch0000003vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m mnist\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/VAE.ipynb#ch0000003vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Input, Reshape, Conv2D, Conv2DTranspose, Flatten, Dense, Lambda\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/__init__.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/__init__.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/__init__.py?line=23'>24</a>\u001b[0m \u001b[39m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/__init__.py?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/__init__.py?line=26'>27</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/__init__.py?line=27'>28</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/models.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/models.py?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/models.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/models.py?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics \u001b[39mas\u001b[39;00m metrics_module\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/models.py?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer_v1\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/models.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m functional\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/metrics.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py?line=23'>24</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py?line=25'>26</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py?line=26'>27</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py?line=27'>28</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/activations.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/activations.py?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/activations.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/activations.py?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m advanced_activations\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/activations.py?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize_keras_object\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/activations.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m serialize_keras_object\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/layers/__init__.py:93\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/__init__.py?line=89'>90</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvolutional\u001b[39;00m \u001b[39mimport\u001b[39;00m Cropping3D\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/__init__.py?line=91'>92</a>\u001b[0m \u001b[39m# Core layers.\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/__init__.py?line=92'>93</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Masking\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/__init__.py?line=93'>94</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Dropout\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/__init__.py?line=94'>95</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m SpatialDropout1D\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/layers/core/__init__.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/__init__.py?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivity_regularization\u001b[39;00m \u001b[39mimport\u001b[39;00m ActivityRegularization\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/__init__.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdense\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/__init__.py?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdropout\u001b[39;00m \u001b[39mimport\u001b[39;00m Dropout\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/__init__.py?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflatten\u001b[39;00m \u001b[39mimport\u001b[39;00m Flatten\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/__init__.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlambda_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Lambda\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m keras_export\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=24'>25</a>\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mkeras.layers.Dropout\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=25'>26</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDropout\u001b[39;00m(base_layer\u001b[39m.\u001b[39;49mBaseRandomLayer):\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=26'>27</a>\u001b[0m   \u001b[39m\"\"\"Applies Dropout to the input.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=27'>28</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=28'>29</a>\u001b[0m \u001b[39m  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=73'>74</a>\u001b[0m \u001b[39m      training mode (adding dropout) or in inference mode (doing nothing).\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=74'>75</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/layers/core/dropout.py?line=76'>77</a>\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, rate, noise_shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, seed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras.engine.base_layer' has no attribute 'BaseRandomLayer'"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","%matplotlib inline\n","\n","from scipy.stats import norm\n","\n","import keras\n","from keras.datasets import mnist\n","from keras.layers import Input, Reshape, Conv2D, Conv2DTranspose, Flatten, Dense, Lambda\n","from keras.models import Model\n","from keras import metrics\n","from keras import backend as K   # 'generic' backend so code works with either tensorflow or theano\n","\n","K.clear_session()\n","np.random.seed(237)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load Dataset\n","(X_train, Y_train), (X_valid, Y_valid) = mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(60000, 28, 28)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape"]},{"cell_type":"markdown","metadata":{},"source":["### dataset description\n","The MNIST dataset is a built-in dataset provided by Keras. It consists of 70,000 28x28 grayscale images, each of which displays a single handwritten digit from 0 to 9. The training set consists of 60,000 images, while the test set has 10,000 images."]},{"cell_type":"markdown","metadata":{"_cell_guid":"012e0e0b-8f35-4722-b6d3-22fd2619c8dd","_uuid":"9caef978723aa28443dc33602e5e096f4535d750"},"source":["### *2. Reshape & normalize*\n","Our encoder and decoder are deep convnets constructed using the Keras Functional API. We'll need to separate the inputs from the labels, normalize them by dividing the max pixel value, and reshape them into 28x28 pixel images."]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"f1e92f03-2b6c-4830-8a3e-e062f9411fdb","_uuid":"f91d57425cd06cde12375a700a423895da9c7c3e","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Normalize and reshape\n","X_train = X_train.astype('float32') / 255.\n","X_train = X_train.reshape(-1,28,28,1)\n","\n","X_valid = X_valid.astype('float32') / 255.\n","X_valid = X_valid.reshape(-1,28,28,1)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"fef54f40-7d3e-4767-bf72-d3b94509b5da","_uuid":"b89e64ddca2b0a11ae4fe9ddfe4b928f60594c14"},"source":["We can take a look at a few random images. The bottom right panel shows one of the more difficult-to-classify digits (even for humans!)."]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"98261ef0-11de-48f3-add1-e914a62674fd","_uuid":"c5f9fd1d7d3e7c4e01d73f039b90309499300b41","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 4 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.figure(1)\n","plt.subplot(221)\n","plt.imshow(X_train[13][:,:,0])\n","\n","plt.subplot(222)\n","plt.imshow(X_train[690][:,:,0])\n","\n","plt.subplot(223)\n","plt.imshow(X_train[2375][:,:,0])\n","\n","plt.subplot(224)\n","plt.imshow(X_train[42013][:,:,0])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"2c02e436-c1c0-4d03-ae48-88253a871e10","_uuid":"5252a23c95a1ad720fe6e0681d10d91624547882"},"source":["## 3. *Model construction*\n","### *A. Encoder network*\n","A VAE has three basic parts:  \n","\n","1. An encoder that that learns the parameters (mean and variance) of the underlying latent distribution;  \n","2. A means of sampling from that distribution; and,  \n","3. A decoder that can turn the sample from #2 back into an image.  \n","\n","In this example, both the encoder and decoder networks are deep convnets. You'll notice that the encoder below has two output layers, one for the latent distribution mean (z_mu) and the other for its variance (z_log_sigma)."]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-18 23:46:29.982344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2022-05-18 23:46:29.985127: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2022-05-18 23:46:29.987004: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-K0HP3GCL): /proc/driver/nvidia/version does not exist\n","2022-05-18 23:46:29.996617: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["img_shape = (28, 28, 1)    # for MNIST\n","batch_size = 16\n","latent_dim = 2  # Number of latent dimension parameters\n","\n","# Encoder architecture: Input -> Conv2D*4 -> Flatten -> Dense\n","input_img = Input(shape=img_shape)\n","\n","x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n","x = Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n","x = Conv2D(64, 3, padding='same', activation='relu')(x)\n","x = Conv2D(64, 3, padding='same', activation='relu')(x)\n","# need to know the shape of the network here for the decoder\n","shape_before_flattening = K.int_shape(x)\n","\n","x = Flatten()(x)\n","x = Dense(32, activation='relu')(x)\n","\n","# Two outputs, latent mean and (log)variance\n","z_mu = Dense(latent_dim)(x)\n","z_log_sigma = Dense(latent_dim)(x)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"93b37058-c9d3-4bfc-b3d3-a0d1aa200965","_uuid":"168c43129e87eec53c6af72dba48c7a761a79063"},"source":["### *B. Sampling function*\n","Next, we create a function to sample from the distribution we just learned the parameters of. `epsilon` is a tensor of small random normal values. One of the assumptions underlying a VAE like this is that our data arose from a random process and is normally distributed in the latent space.\n","\n","With Keras, everything has to be in a 'layer' to compile correctly. This goes for our sampling function. The `Lambda` layer wrapper let's us do this."]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"c8e1ba8f-a88a-4687-80f3-68f6815bb032","_uuid":"b8a145b6db334a68977c89639a08ef371e54cdd6","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# sampling function\n","def sampling(args):\n","    z_mu, z_log_sigma = args\n","    epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim),\n","                              mean=0., stddev=1.)\n","    return z_mu + K.exp(z_log_sigma) * epsilon\n","\n","# sample vector from the latent distribution\n","z = Lambda(sampling)([z_mu, z_log_sigma])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"3442d1be-3e02-44db-b615-59b010089776","_uuid":"d6f83916df89ac4afeee046fc2d3f817fc16c249"},"source":["### *C. Decoder network*\n","The decoder is basically the encoder in reverse."]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"68322de7-9675-4788-9c60-9044cb3172db","_uuid":"0df3d7471211f7403735a4e923c47a2294a0a349","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# decoder takes the latent distribution sample as input\n","decoder_input = Input(K.int_shape(z)[1:])\n","\n","# Expand to 784 total pixels\n","x = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(decoder_input)\n","\n","# reshape\n","x = Reshape(shape_before_flattening[1:])(x)\n","\n","# use Conv2DTranspose to reverse the conv layers from the encoder\n","x = Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2, 2))(x)\n","x = Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n","\n","# decoder model statement\n","decoder = Model(decoder_input, x)\n","\n","# apply the decoder to the sample from the latent distribution\n","z_decoded = decoder(z)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f93644c4-28cd-43dc-8e1c-f8c11b60be90","_uuid":"705e156f13e4b89c73c2d5c06f0b0965d74a182f"},"source":["### *D. Loss*\n","We need one more thing and that's something that will calculate the unique loss function the VAE requires. Recall that the VAE is trained using a loss function with two components:  \n","\n","1. 'Reconstruction loss' - This is the cross-entropy describing the errors between the decoded samples from the latent distribution and the original inputs.  \n","2. The Kullback-Liebler divergence between the latent distribution and the prior (this acts as a sort of regularization term).  \n","\n","We define a custom layer class that calculates the loss. "]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"394127bc-7beb-406a-8e2c-7aae3b436d85","_uuid":"289b40920037fbdb305d3e76a5a08495a143693c","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# construct a custom layer to calculate the loss\n","class CustomVariationalLayer(keras.layers.Layer):\n","\n","    def vae_loss(self, x, z_decoded):\n","        x = K.flatten(x)\n","        z_decoded = K.flatten(z_decoded)\n","        # Reconstruction loss\n","        xent_loss = metrics.binary_crossentropy(x, z_decoded)\n","        # KL divergence\n","        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n","        return K.mean(xent_loss + kl_loss)\n","\n","    # adds the custom loss to the class\n","    def call(self, inputs):\n","        x = inputs[0]\n","        z_decoded = inputs[1]\n","        loss = self.vae_loss(x, z_decoded)\n","        self.add_loss(loss, inputs=inputs)\n","        return x\n","\n","# apply the custom loss to the input images and the decoded latent distribution sample\n","y = CustomVariationalLayer()([input_img, z_decoded])"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b56e4d33-5d1a-49c2-aa3e-c00692862889","_uuid":"80350c12746fdab9892122af1b60248f7567f0fe"},"source":["Now we can instantiate the model and take a look at its summary."]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b189196e-deb3-429d-bf3b-1c8c6d4a4afb","_uuid":"91449af9f139e7080ddaa8b0a8152b66b9d02d95","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 28, 28, 32)   320         ['input_1[0][0]']                \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 14, 14, 64)   18496       ['conv2d[0][0]']                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_1[0][0]']               \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_2[0][0]']               \n","                                                                                                  \n"," flatten (Flatten)              (None, 12544)        0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," dense (Dense)                  (None, 32)           401440      ['flatten[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)                (None, 2)            66          ['dense[0][0]']                  \n","                                                                                                  \n"," dense_2 (Dense)                (None, 2)            66          ['dense[0][0]']                  \n","                                                                                                  \n"," lambda (Lambda)                (None, 2)            0           ['dense_1[0][0]',                \n","                                                                  'dense_2[0][0]']                \n","                                                                                                  \n"," model (Functional)             (None, 28, 28, 1)    56385       ['lambda[0][0]']                 \n","                                                                                                  \n"," custom_variational_layer (Cust  (None, 28, 28, 1)   0           ['input_1[0][0]',                \n"," omVariationalLayer)                                              'model[0][0]']                  \n","                                                                                                  \n","==================================================================================================\n","Total params: 550,629\n","Trainable params: 550,629\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# VAE model statement\n","vae = Model(input_img, y)\n","vae.compile(optimizer='rmsprop', loss=None)\n","vae.summary()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"cf0514db-2761-40b7-b174-7f40d8112d91","_uuid":"a838351a7a4aa74c8a1a50d09a4a4a3594aaaac1"},"source":["## 4. *Train the VAE*\n","Finally, we fit the model."]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["num_epochs = 10"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.5.0\n"]}],"source":["import tensorflow as tf\n","print(tf.version.VERSION)"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"d44843b2-3e0d-4319-bc6e-8089871ec1ea","_uuid":"e052652709cbd5fcb9e4a836306245edaaf0ab51","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-05-18 23:46:36.577360: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"ename":"TypeError","evalue":"in user code:\n\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 239, in __call__\n        self._loss_metric.update_state(\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py\", line 178, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py\", line 471, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/keras_tensor.py\", line 254, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/VAE.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/VAE.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m vae\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mX_train, y\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epochs\u001b[39m=\u001b[39;49mnum_epochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, validation_data\u001b[39m=\u001b[39;49m(X_valid, \u001b[39mNone\u001b[39;49;00m))\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/cyy/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/cyy/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/cyy/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/cyy/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///home/cyy/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/cyy/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 239, in __call__\n        self._loss_metric.update_state(\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py\", line 178, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/metrics.py\", line 471, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"/home/cyy/.local/lib/python3.9/site-packages/keras/engine/keras_tensor.py\", line 254, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"]}],"source":["vae.fit(x=X_train, y=None, shuffle=True, epochs=num_epochs, batch_size=batch_size, validation_data=(X_valid, None))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"8a3a7177-31bc-462b-a43f-d79dfd79f654","_uuid":"3e03487ab62f3f8807441ca6501438b74a5c95c6"},"source":["## 5. *Results*  \n","### *A. Clustering of digits in the latent space*\n","We can make predictions on the validation set using the encoder network. This has the effect of translating the images from the 784-dimensional input space into the 2-dimensional latent space. When we color-code those translated data points according to their *known* digit class, we can see how the digits cluster together."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afe4a541-814e-4def-88cd-cbee97e18dfe","_uuid":"0d8c22694d59990b52efe2c1a6cac66b9d899412","trusted":true},"outputs":[],"source":["# Translate into the latent space\n","encoder = Model(input_img, z_mu)\n","X_valid_encoded = encoder.predict(X_valid, batch_size=batch_size)\n","plt.figure(figsize=(10, 10))\n","plt.scatter(X_valid_encoded[:, 0], X_valid_encoded[:, 1], c=Y_valid, cmap='brg')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"508868ea-ee14-4a56-9207-d4ac2607697d","_uuid":"1c36beb9eae66ebc216ba9a8e7ee9994e776a530"},"source":["Including the original test set data lets us see where they fall with respect to the known digit clusters."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e53af49f-8591-403d-a909-5bb54947d040","_uuid":"5a1bb733976c79bf74a3a9cafa262d73e1b93865","trusted":true},"outputs":[],"source":["# set colormap so that 11's are gray\n","custom_cmap = matplotlib.cm.get_cmap('brg')\n","custom_cmap.set_over('gray')\n","\n","X_valid_encoded = encoder.predict(X_valid, batch_size=batch_size)\n","plt.figure(figsize=(10, 10))\n","gray_marker = mpatches.Circle(4,radius=0.1,color='gray', label='Test')\n","plt.legend(handles=[gray_marker], loc = 'best')\n","plt.scatter(X_valid_encoded[:, 0], X_valid_encoded[:, 1], c=Y_valid, cmap=custom_cmap)\n","plt.clim(0, 9)\n","plt.colorbar()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"f592181f-07a4-4fc8-aee7-2f85922b1a1b","_uuid":"277873274ca79a7e5abc4243609c0a169ca6c87b"},"source":["### *B. Sample digits*\n","Another fun thing we can do is to use the decoder network to take a peak at what samples from the latent space look like as we change the latent variables. What we end up with is a smoothly varying space where each digit transforms into the others as we dial the latent variables up and down."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b6e0f3e-eb67-446d-aa17-492b1c68077a","_uuid":"ab1bcf744f65020eef6665b2166d32820842b824","trusted":true},"outputs":[],"source":["# Display a 2D manifold of the digits\n","n = 20  # figure with 20x20 digits\n","digit_size = 28\n","figure = np.zeros((digit_size * n, digit_size * n))\n","\n","# Construct grid of latent variable values\n","grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n","grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n","\n","# decode for each square in the grid\n","for i, yi in enumerate(grid_x):\n","    for j, xi in enumerate(grid_y):\n","        Z_sample = np.array([[xi, yi]])\n","        Z_sample = np.tile(Z_sample, batch_size).reshape(batch_size, 2)\n","        X_decoded = decoder.predict(Z_sample, batch_size=batch_size)\n","        digit = X_decoded[0].reshape(digit_size, digit_size)\n","        figure[i * digit_size: (i + 1) * digit_size,\n","               j * digit_size: (j + 1) * digit_size] = digit\n","\n","plt.figure(figsize=(10, 10))\n","plt.imshow(figure, cmap='gnuplot2')\n","plt.show()  "]}],"metadata":{"interpreter":{"hash":"f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
