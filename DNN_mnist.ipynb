{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11191\\Desktop\\2022 Summer\\Out of distribution learning\\code\\DNN_mnist.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/DNN_mnist.ipynb#ch0000000?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/DNN_mnist.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn, optim\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/DNN_mnist.ipynb#ch0000000?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/DNN_mnist.ipynb#ch0000000?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/DNN_mnist.ipynb#ch0000000?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        self.image = torch.tensor(image, dtype = torch.float32)\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self): return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_ = self.image[idx]\n",
    "        image_ = image_[None, :]\n",
    "        label_ = self.label[idx]\n",
    "        return image_, label_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_tr, y_tr), (x_ts, y_ts) = keras.datasets.mnist.load_data()\n",
    "trainset = MNISTDataset(x_tr, y_tr)\n",
    "valset = MNISTDataset(x_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(trainloader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def nth_derivative(f, wrt, n):\n",
    "    for i in range(n):\n",
    "        # f.retain_grad()\n",
    "        # grads = grad(f, wrt, create_graph=True)[0]\n",
    "        grads = grad(f, wrt,create_graph=True,allow_unused=True)[0]\n",
    "        \n",
    "        # grads = grad(f, wrt, create_graph=True,allow_unused=True)[0]\n",
    "        # print(grad(f, wrt, create_graph=True,allow_unused=True))\n",
    "        # print(grads)\n",
    "        f = grads.sum()\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(trainloader):\n",
    "    data.requires_grad_(True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    # print(output.shape) # torch.Size([64, 10])\n",
    "    # print(target.shape) # torch.Size([64])\n",
    "\n",
    "    # loss = F.nll_loss(output, target) + 0.001 * nth_derivative(output,data,2)\n",
    "    second_derivative = None\n",
    "    if second_derivative == None:\n",
    "      second_derivative = 0\n",
    "    loss = F.nll_loss(output, target) + second_derivative\n",
    "    temp_scalar_out = np.sqrt(np.sum(np.square(output.detach().numpy())))\n",
    "    scalar_out = torch.tensor(temp_scalar_out, requires_grad=True)\n",
    "    loss.backward()\n",
    "    # first_derivative = grad(scalar_out, data, create_graph=True, allow_unused=True)[0]\n",
    "    # second_derivative = grad(first_derivative, data, allow_unused=True)[0]\n",
    "    # print(second_derivative)\n",
    "    first_derivative = data.grad\n",
    "    first_derivative = torch.tensor(first_derivative, requires_grad=True)\n",
    "    print(first_derivative.shape)\n",
    "    second_derivative = first_derivative.grad\n",
    "    print(second_derivative)\n",
    "    \n",
    "    # print(nth_derivative(scalar_out, data, 2))\n",
    "    # print(nth_derivative(output, data, 2))\n",
    "\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "        100. * batch_idx / len(trainloader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "         (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))\n",
    "      # torch.save(network.state_dict(), '/results/model.pth')\n",
    "      # torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in valloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(valloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valloader.dataset),\n",
    "    100. * correct / len(valloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16001/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.2596, Accuracy: 9331/10000 (93%)\n",
      "\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.577593\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16001/2152249556.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  first_derivative = torch.tensor(first_derivative, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n",
      "torch.Size([64, 1, 28, 28])\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m test()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m   train(epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m   test()\n",
      "\u001b[1;32m/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(epoch):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m   network\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m   \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000010vscode-remote?line=5'>6</a>\u001b[0m     data\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Users/tianyueli/cyy/DNN_mnist.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/tftorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/tftorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/tftorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/tftorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tftorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tftorch/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    136\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[1;32m    137\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m    139\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    140\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(loader):\n",
    "    network.eval()\n",
    "    # outputs = [] \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = network(data)\n",
    "            # outputs.append(output)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loader.dataset),\n",
    "        100. * correct / len(loader.dataset)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=False)\n",
    "tsloader = torch.utils.data.DataLoader(valset, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_964/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1178, Accuracy: 57779/60000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1224, Accuracy: 9613/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_score = scores(trloader)\n",
    "test_score = scores(tsloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_score, 'train_score.pt')\n",
    "torch.save(test_score, 'test_score.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = torch.load('train_score.pt')\n",
    "train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr[0:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_tr[0:1000], train_score[0:1000, 0])\n",
    "plt.show()\n",
    "# torch.scatter(train_score[0:200, 0], index = torch.from_numpy(y_tr[0:200]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "167f855b06db51e9095a29fe31162bca6945ad2d479365fff88adf921f75ee7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
