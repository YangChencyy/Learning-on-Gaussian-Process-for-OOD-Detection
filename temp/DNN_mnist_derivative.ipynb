{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyy/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-05 10:45:24.946115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-05 10:45:24.946742: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        #self.image = image.clone().detach # torch.tensor(image, dtype = torch.float32)\n",
    "        self.image  = image.to(torch.float32)\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self): return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_ = self.image[idx]\n",
    "        image_ = image_[None, :]\n",
    "        label_ = self.label[idx]\n",
    "        return image_, label_\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_ts, y_ts) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6400, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# selecting only label 0 to 5\n",
    "x, y = [], []\n",
    "for i in range(len(x_tr)):\n",
    "    if y_tr[i] < 5:\n",
    "        x.append(x_tr[i])\n",
    "        y.append(y_tr[i])\n",
    "x_tr, y_tr = torch.from_numpy(np.stack(x)), torch.from_numpy(np.stack(y))\n",
    "x, y = [], []\n",
    "for i in range(len(x_ts)):\n",
    "    if y_ts[i] < 5:\n",
    "        x.append(x_ts[i])\n",
    "        y.append(y_ts[i])\n",
    "x_ts, y_ts = torch.from_numpy(np.stack(x)), torch.from_numpy(np.stack(y))\n",
    "print(x_tr.shape)\n",
    "\n",
    "x_tr, y_tr = x_tr[0:6400], y_tr[0:6400]\n",
    "x_ts, y_ts = x_ts[0:640], y_ts[0:640]\n",
    "trainset = MNISTDataset(x_tr, y_tr)\n",
    "valset = MNISTDataset(x_ts, y_ts)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(75)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)   # change according to the num of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1    # change the epoch\n",
    "learning_rate = 0.006\n",
    "momentum = 0.5\n",
    "log_interval = 1  # change for output\n",
    "\n",
    "networks, opts = [], []\n",
    "\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(trainloader.dataset) for i in range(n_epochs + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher(shape, device='cpu'):\n",
    "    \"\"\"Creates a random tensor of size [shape] under the Rademacher distribution (P(x=1) == P(x=-1) == 0.5)\"\"\"\n",
    "    x = torch.empty(shape, device=device)\n",
    "    x.random_(0, 2)  # Creates random tensor of 0s and 1s\n",
    "    x[x == 0] = -1  # Turn the 0s into -1s\n",
    "    return x\n",
    "\n",
    "def listify(x):\n",
    "    \"\"\"If x is already a list, do nothing. Otherwise, wrap x in a list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "def multi_layer_second_directional_derivative(G, z, x, G_z, epsilon, **G_kwargs):\n",
    "    \"\"\"Estimates the second directional derivative of G w.r.t. its input at z in the direction x\"\"\"\n",
    "    G_to_x = G(z + x, **G_kwargs)\n",
    "    G_from_x = G(z - x, **G_kwargs)\n",
    "\n",
    "    G_to_x = listify(G_to_x)\n",
    "    G_from_x = listify(G_from_x)\n",
    "    G_z = listify(G_z)\n",
    "\n",
    "    eps_sqr = epsilon ** 2\n",
    "    sdd = [(G2x - 2 * G_z_base + Gfx) / eps_sqr for G2x, G_z_base, Gfx in zip(G_to_x, G_z, G_from_x)]\n",
    "    return sdd\n",
    "\n",
    "def stack_var_and_reduce(list_of_activations, reduction=torch.max):\n",
    "    \"\"\"Equation (5) from the paper.\"\"\"\n",
    "    second_orders = torch.stack(list_of_activations)  # (k, N, C, H, W)\n",
    "    var_tensor = torch.var(second_orders, dim=0, unbiased=True)  # (N, C, H, W)\n",
    "    penalty = reduction(var_tensor)  # (1,) (scalar)\n",
    "    return penalty\n",
    "\n",
    "\n",
    "def multi_stack_var_and_reduce(sdds, reduction=torch.max, return_separately=False):\n",
    "    \"\"\"Iterate over all activations to be regularized, then apply Equation (5) to each.\"\"\"\n",
    "    sum_of_penalties = 0 if not return_separately else []\n",
    "    for activ_n in zip(*sdds):\n",
    "        penalty = stack_var_and_reduce(activ_n, reduction)\n",
    "        sum_of_penalties += penalty if not return_separately else [penalty]\n",
    "    return sum_of_penalties\n",
    "\n",
    "def hessian_penalty(G, z, k=2, epsilon=0.1, reduction=torch.max, return_separately=False, G_z=None, **G_kwargs):\n",
    "    if G_z is None:\n",
    "        G_z = G(z, **G_kwargs)\n",
    "    rademacher_size = torch.Size((k, *z.size()))  # (k, N, z.size())\n",
    "    xs = epsilon * rademacher(rademacher_size, device=z.device)\n",
    "    second_orders = []\n",
    "    for x in xs:  # Iterate over each (N, z.size()) tensor in xs\n",
    "        central_second_order = multi_layer_second_directional_derivative(G, z, x, G_z, epsilon, **G_kwargs)\n",
    "        second_orders.append(central_second_order)  # Appends a tensor with shape equal to G(z).size()\n",
    "    loss = multi_stack_var_and_reduce(second_orders, reduction, return_separately)  # (k, G(z).size()) --> scalar\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative = [[]]*10\n",
    "\n",
    "def train(epoch, para = 10):\n",
    "  \n",
    "  i = 0\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(trainloader):\n",
    "    data.requires_grad_(True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    # print(data.shape)  # [[64, 1, 28, 28]]\n",
    "    # print(output.shape) # [64, 10]\n",
    "    \n",
    "    second_derivative_loss = 0\n",
    "    for r in range(len(data)): # iterate all batches\n",
    "          for j in range(5):  # change for num of labels\n",
    "                if j != y_tr[i]:\n",
    "                      def f(sample):\n",
    "                            res = network(sample)\n",
    "                        #     print(res.shape) [1, 10]\n",
    "                            res = res.view(5)  # change for num of labels\n",
    "                            return res[j]\n",
    "                      H_matrix = torch.autograd.functional.hessian(f, data[r])\n",
    "                  #     print(H_matrix.shape)  [1, 28, 28, 1, 28, 28]\n",
    "                      H_matrix = H_matrix.view(28, 28, 28, 28)\n",
    "                      trace = 0\n",
    "                      for a in range(28):\n",
    "                            for b in range(28):\n",
    "                                  trace += abs(H_matrix[a][b][a][b])  #abs\n",
    "                      second_derivative_loss += trace\n",
    "                \n",
    "    \n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "\n",
    "    second_derivative_loss = para*second_derivative_loss\n",
    "    loss += second_derivative_loss\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print(\"derivative loss:\", second_derivative_loss)\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "        100. * batch_idx / len(trainloader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "         (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))\n",
    "    \n",
    "    # derivative = new_derivative\n",
    "    # new_derivative = []*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in valloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(valloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valloader.dataset),\n",
    "    100. * correct / len(valloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test()\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#   train(epoch)\n",
    "#   test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features_UMAP.npy', 'rb') as f:\n",
    "    train_x = np.load(f)\n",
    "    train_y_label = np.load(f)\n",
    "    test_x = np.load(f)\n",
    "    test_y = np.load(f)\n",
    "    \n",
    "train_x, train_y_label, test_x, test_y = \\\n",
    "    torch.from_numpy(train_x), torch.from_numpy(train_y_label), \\\n",
    "        torch.from_numpy(test_x), torch.from_numpy(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # self.covar_module = gpytorch.kernels.ScaleKernel(RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.MaternKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(loader):\n",
    "    network.eval()\n",
    "    # outputs = [] \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = network(data)\n",
    "            # outputs.append(output)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loader.dataset),\n",
    "        100. * correct / len(loader.dataset)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "all_index = np.arange(0, n)\n",
    "models, likelihoods, mlls, opts = [], [], [], []\n",
    "lr = 0.1\n",
    "\n",
    "def model_initialize_GP():\n",
    "\n",
    "\n",
    "    for j in range(5): # change for num of labels\n",
    "        mo, l, ml, o = [], [], [], []\n",
    "        for k in range(5):  # change for num of labels\n",
    "            val_index = np.arange(np.floor(n/5)*k, np.floor(n/5)*(k+1))  # change for num of labels\n",
    "            train_index = [i for i in all_index if i not in val_index]\n",
    "            x = train_x[train_index, :]\n",
    "            y = log_y[train_index, j]\n",
    "            \n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "            model = ExactGPModel(x, y, likelihood)\n",
    "            likelihood.train()\n",
    "            model.train()\n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "            \n",
    "            l.append(likelihood)\n",
    "            mo.append(model)\n",
    "            ml.append(mll)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = lr)  # Includes GaussianLikelihood parameters\n",
    "            o.append(optimizer)\n",
    "            \n",
    "        models.append(mo)\n",
    "        likelihoods.append(l)\n",
    "        mlls.append(ml)\n",
    "        opts.append(o)\n",
    "        \n",
    "    # models[0][1] represents label 0 and the first fold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_y = []\n",
    "def train_GP():\n",
    "    training_iter = 10\n",
    "    # train_y = torch.load('train_score.pt') : x axis\n",
    "\n",
    "    for j in range(5):  # j represents the 10 labels # change for num of labels\n",
    "        p = []\n",
    "        for k in range(5): # k represents the k fold cross validation  # change for num of labels\n",
    "            print(\"Model\", j, \", Cross Validation Group\", k)\n",
    "            # initialize the model\n",
    "            val_index = np.arange(np.floor(n/5)*k, np.floor(n/5)*(k+1))  # change for num of labels\n",
    "            train_index = [i for i in all_index if i not in val_index]\n",
    "            tr_x = train_x[train_index, :]\n",
    "            val_x = train_x[val_index, :]\n",
    "            tr_y = log_y[train_index, j]\n",
    "            # val_y = train_y[val_index, k]\n",
    "            likelihood = likelihoods[j][k]\n",
    "            model = models[j][k]\n",
    "            mll = mlls[j][k]\n",
    "            optimizer = opts[j][k]\n",
    "            \n",
    "            # training\n",
    "            for i in range(training_iter):                \n",
    "                # Zero gradients from previous iteration\n",
    "                optimizer.zero_grad()\n",
    "                # Output from model\n",
    "                output = model(tr_x)\n",
    "                # print(output.shape, tr_y.shape)\n",
    "                \n",
    "                # Calc loss and backprop gradients\n",
    "                loss = -mll(output, tr_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                if i % 4 == 0:\n",
    "                    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                        i + 1, training_iter, loss.item(),\n",
    "                        #model.covar_module.base_kernel.lengthscale.item(),\n",
    "                        0.1,\n",
    "                        model.likelihood.noise.item()\n",
    "                    ))\n",
    "                optimizer.step()\n",
    "            \n",
    "                # evaluation of the current model\n",
    "            model.eval()\n",
    "            likelihood.eval()\n",
    "            with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "                test_dist = model(val_x)\n",
    "                pred_means = test_dist.loc\n",
    "                p.append(pred_means)\n",
    "        p = torch.stack(p)\n",
    "        pred_train_y.append(p)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1772/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 6.7439, Accuracy: 27/640 (4%)\n",
      "\n",
      "derivative loss: tensor(-0.0697)\n",
      "Train Epoch: 1 [0/6400 (0%)]\tLoss: 35.723858\n",
      "derivative loss: tensor(-0.0216)\n",
      "Train Epoch: 1 [64/6400 (1%)]\tLoss: 29.912062\n",
      "derivative loss: tensor(-1.2460)\n",
      "Train Epoch: 1 [128/6400 (2%)]\tLoss: 1.219437\n",
      "derivative loss: tensor(-3.0843)\n",
      "Train Epoch: 1 [192/6400 (3%)]\tLoss: -1.065788\n",
      "derivative loss: tensor(-2.3277)\n",
      "Train Epoch: 1 [256/6400 (4%)]\tLoss: -0.169231\n",
      "derivative loss: tensor(-2.4885)\n",
      "Train Epoch: 1 [320/6400 (5%)]\tLoss: -0.421961\n",
      "derivative loss: tensor(-2.6261)\n",
      "Train Epoch: 1 [384/6400 (6%)]\tLoss: -0.552436\n",
      "derivative loss: tensor(-1.6370)\n",
      "Train Epoch: 1 [448/6400 (7%)]\tLoss: 0.275743\n",
      "derivative loss: tensor(-2.2441)\n",
      "Train Epoch: 1 [512/6400 (8%)]\tLoss: -0.404477\n",
      "derivative loss: tensor(-1.8840)\n",
      "Train Epoch: 1 [576/6400 (9%)]\tLoss: -0.027307\n",
      "derivative loss: tensor(-1.7580)\n",
      "Train Epoch: 1 [640/6400 (10%)]\tLoss: 0.182201\n",
      "derivative loss: tensor(-1.7440)\n",
      "Train Epoch: 1 [704/6400 (11%)]\tLoss: 0.028810\n",
      "derivative loss: tensor(-1.5474)\n",
      "Train Epoch: 1 [768/6400 (12%)]\tLoss: 0.313864\n",
      "derivative loss: tensor(-1.7565)\n",
      "Train Epoch: 1 [832/6400 (13%)]\tLoss: -0.030214\n",
      "derivative loss: tensor(-1.9449)\n",
      "Train Epoch: 1 [896/6400 (14%)]\tLoss: -0.272214\n",
      "derivative loss: tensor(-1.7681)\n",
      "Train Epoch: 1 [960/6400 (15%)]\tLoss: -0.087600\n",
      "derivative loss: tensor(-1.8963)\n",
      "Train Epoch: 1 [1024/6400 (16%)]\tLoss: -0.120858\n",
      "derivative loss: tensor(-1.9685)\n",
      "Train Epoch: 1 [1088/6400 (17%)]\tLoss: -0.664577\n",
      "derivative loss: tensor(-1.4968)\n",
      "Train Epoch: 1 [1152/6400 (18%)]\tLoss: -0.050278\n",
      "derivative loss: tensor(-1.0606)\n",
      "Train Epoch: 1 [1216/6400 (19%)]\tLoss: 0.442286\n",
      "derivative loss: tensor(-1.3535)\n",
      "Train Epoch: 1 [1280/6400 (20%)]\tLoss: 0.082390\n",
      "derivative loss: tensor(-1.6201)\n",
      "Train Epoch: 1 [1344/6400 (21%)]\tLoss: -0.123928\n",
      "derivative loss: tensor(-1.7376)\n",
      "Train Epoch: 1 [1408/6400 (22%)]\tLoss: -0.459703\n",
      "derivative loss: tensor(-1.1058)\n",
      "Train Epoch: 1 [1472/6400 (23%)]\tLoss: -0.034479\n",
      "derivative loss: tensor(-0.8813)\n",
      "Train Epoch: 1 [1536/6400 (24%)]\tLoss: 0.305197\n",
      "derivative loss: tensor(-1.0740)\n",
      "Train Epoch: 1 [1600/6400 (25%)]\tLoss: 0.359774\n",
      "derivative loss: tensor(-0.8546)\n",
      "Train Epoch: 1 [1664/6400 (26%)]\tLoss: 0.394985\n",
      "derivative loss: tensor(-1.0597)\n",
      "Train Epoch: 1 [1728/6400 (27%)]\tLoss: 0.091120\n",
      "derivative loss: tensor(-1.1281)\n",
      "Train Epoch: 1 [1792/6400 (28%)]\tLoss: -0.030797\n",
      "derivative loss: tensor(-1.0651)\n",
      "Train Epoch: 1 [1856/6400 (29%)]\tLoss: 0.158685\n",
      "derivative loss: tensor(-0.9297)\n",
      "Train Epoch: 1 [1920/6400 (30%)]\tLoss: 0.358641\n",
      "derivative loss: tensor(-0.8656)\n",
      "Train Epoch: 1 [1984/6400 (31%)]\tLoss: 0.330627\n",
      "derivative loss: tensor(-0.6675)\n",
      "Train Epoch: 1 [2048/6400 (32%)]\tLoss: 0.594108\n",
      "derivative loss: tensor(-0.7475)\n",
      "Train Epoch: 1 [2112/6400 (33%)]\tLoss: 0.733865\n",
      "derivative loss: tensor(-0.4481)\n",
      "Train Epoch: 1 [2176/6400 (34%)]\tLoss: 0.520682\n",
      "derivative loss: tensor(-0.7171)\n",
      "Train Epoch: 1 [2240/6400 (35%)]\tLoss: 0.441155\n",
      "derivative loss: tensor(-0.5260)\n",
      "Train Epoch: 1 [2304/6400 (36%)]\tLoss: 0.513336\n",
      "derivative loss: tensor(-0.7412)\n",
      "Train Epoch: 1 [2368/6400 (37%)]\tLoss: 0.543181\n",
      "derivative loss: tensor(-0.7429)\n",
      "Train Epoch: 1 [2432/6400 (38%)]\tLoss: 0.437585\n",
      "derivative loss: tensor(-0.7203)\n",
      "Train Epoch: 1 [2496/6400 (39%)]\tLoss: 0.500870\n",
      "derivative loss: tensor(-0.4027)\n",
      "Train Epoch: 1 [2560/6400 (40%)]\tLoss: 0.701670\n",
      "derivative loss: tensor(-0.6664)\n",
      "Train Epoch: 1 [2624/6400 (41%)]\tLoss: 0.609399\n",
      "derivative loss: tensor(-0.8130)\n",
      "Train Epoch: 1 [2688/6400 (42%)]\tLoss: 0.331894\n",
      "derivative loss: tensor(-0.6207)\n",
      "Train Epoch: 1 [2752/6400 (43%)]\tLoss: 0.452316\n",
      "derivative loss: tensor(-0.6844)\n",
      "Train Epoch: 1 [2816/6400 (44%)]\tLoss: 0.272106\n",
      "derivative loss: tensor(-0.6412)\n",
      "Train Epoch: 1 [2880/6400 (45%)]\tLoss: 0.274535\n",
      "derivative loss: tensor(-0.6635)\n",
      "Train Epoch: 1 [2944/6400 (46%)]\tLoss: 0.386674\n",
      "derivative loss: tensor(-0.5588)\n",
      "Train Epoch: 1 [3008/6400 (47%)]\tLoss: 0.422331\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/temp/DNN_mnist_derivative.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000013vscode-remote?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000013vscode-remote?line=7'>8</a>\u001b[0m                   momentum\u001b[39m=\u001b[39mmomentum)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000013vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000013vscode-remote?line=9'>10</a>\u001b[0m     train(epoch, para)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000013vscode-remote?line=10'>11</a>\u001b[0m     test()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000013vscode-remote?line=12'>13</a>\u001b[0m trloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(trainset, batch_size\u001b[39m=\u001b[39m\u001b[39m6400\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/temp/DNN_mnist_derivative.ipynb Cell 6'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, para)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000005vscode-remote?line=21'>22</a>\u001b[0m           res \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mview(\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000005vscode-remote?line=22'>23</a>\u001b[0m           \u001b[39mreturn\u001b[39;00m res[j]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000005vscode-remote?line=23'>24</a>\u001b[0m     H_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mhessian(f, data[r])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000005vscode-remote?line=24'>25</a>\u001b[0m \u001b[39m#     print(H_matrix.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/temp/DNN_mnist_derivative.ipynb#ch0000005vscode-remote?line=25'>26</a>\u001b[0m     H_matrix \u001b[39m=\u001b[39m H_matrix\u001b[39m.\u001b[39mview(\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/functional.py:807\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    805\u001b[0m     \u001b[39mreturn\u001b[39;00m jac\n\u001b[0;32m--> 807\u001b[0m res \u001b[39m=\u001b[39m jacobian(jac_func, inputs, create_graph\u001b[39m=\u001b[39;49mcreate_graph, strict\u001b[39m=\u001b[39;49mstrict, vectorize\u001b[39m=\u001b[39;49mvectorize,\n\u001b[1;32m    808\u001b[0m                strategy\u001b[39m=\u001b[39;49mouter_jacobian_strategy)\n\u001b[1;32m    809\u001b[0m \u001b[39mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/functional.py:669\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    667\u001b[0m jac_i: Tuple[List[torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inputs)))  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(out\u001b[39m.\u001b[39mnelement()):\n\u001b[0;32m--> 669\u001b[0m     vj \u001b[39m=\u001b[39m _autograd_grad((out\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[j],), inputs,\n\u001b[1;32m    670\u001b[0m                         retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49mcreate_graph)\n\u001b[1;32m    672\u001b[0m     \u001b[39mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    673\u001b[0m         \u001b[39mif\u001b[39;00m vj_el \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m,) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(inputs)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(new_outputs, inputs, new_grad_outputs, allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    160\u001b[0m                                create_graph\u001b[39m=\u001b[39;49mcreate_graph, retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    161\u001b[0m                                is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs)\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m         outputs, grad_outputs_, retain_graph, create_graph, inputs,\n\u001b[1;32m    277\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test()\n",
    "parameters = [0.1, 1, 10, 100]\n",
    "for para in parameters:\n",
    "    \n",
    "    \n",
    "    network = Net()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(epoch, para)\n",
    "        test()\n",
    "        \n",
    "    trloader = torch.utils.data.DataLoader(trainset, batch_size=6400, shuffle=False)\n",
    "    tsloader = torch.utils.data.DataLoader(valset, batch_size=640, shuffle=False)\n",
    "    train_score = scores(trloader)\n",
    "    # test_score = scores(tsloader)\n",
    "    filename = \"data/para\" + str(para) + '_train_score.pt'\n",
    "    torch.save(train_score, filename)\n",
    "    # torch.save(test_score, 'test_score.pt')\n",
    "    \n",
    "    train_y = torch.load(filename)\n",
    "    \n",
    "    train_x, train_y = train_x[0: n], train_y[0: n, :]\n",
    "    test_y = test_y.to(torch.int64)\n",
    "    add = - torch.min(train_y) + 1\n",
    "    log_y = add + train_y\n",
    "    log_y = torch.log(log_y)\n",
    "    \n",
    "    \n",
    "    models, likelihoods, mlls, opts = [], [], [], []\n",
    "    model_initialize_GP()\n",
    "    pred_train_y = []\n",
    "    train_GP()\n",
    "    \n",
    "    \n",
    "    for j in range(5):   # change for num of labels\n",
    "        pred_train_y[j] = pred_train_y[j].view(1, -1).flatten()\n",
    "        pred_train_y[j] = torch.exp(pred_train_y[j]) - add\n",
    "        \n",
    "    for j in range(5):     # change for num of labels\n",
    "        name = \"plots/para\" + str(para) + \"scaled_model\" + str(j) +\".png\"\n",
    "        x_data = train_y[:, j]\n",
    "        mini = torch.min(x_data)\n",
    "        if mini < -20:\n",
    "            mini = -40\n",
    "        else: \n",
    "            mini = -20\n",
    "        y_data= pred_train_y[j]\n",
    "        plt.plot(x_data, y_data, 'o', color='black')\n",
    "        x = np.linspace(-10,2,100)\n",
    "        y = x\n",
    "        plt.plot(x, y, '-r', label='y=2x+1')\n",
    "        plt.title('Model' + str(j))\n",
    "        plt.xlabel('DNN training scores', color='#1C2833')\n",
    "        plt.ylabel('GP training scores', color='#1C2833')\n",
    "        plt.savefig(name, facecolor = 'white', transparent = False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(y_tr[0:500], train_score[0:500, 0])\n",
    "# plt.show()\n",
    "# torch.scatter(train_score[0:200, 0], index = torch.from_numpy(y_tr[0:200]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
