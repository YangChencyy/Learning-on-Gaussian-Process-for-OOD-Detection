{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyy/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-24 08:48:42.196817: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-24 08:48:42.197023: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        self.image = torch.tensor(image, dtype = torch.float32)\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self): return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_ = self.image[idx]\n",
    "        image_ = image_[None, :]\n",
    "        label_ = self.label[idx]\n",
    "        return image_, label_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_ts, y_ts) = keras.datasets.mnist.load_data()\n",
    "trainset = MNISTDataset(x_tr, y_tr)\n",
    "valset = MNISTDataset(x_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(trainloader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import grad\n",
    "\n",
    "# def nth_derivative(f, wrt, n):\n",
    "\n",
    "#     for i in range(n):\n",
    "\n",
    "#         grads = grad(f, wrt, create_graph=True)[0]\n",
    "#         f = grads.sum()\n",
    "\n",
    "#     return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(trainloader):\n",
    "    data.requires_grad_(True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    # print(output.shape)\n",
    "    \n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    first_derivative = data.grad\n",
    "    print(first_derivative.shape)\n",
    "\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "        100. * batch_idx / len(trainloader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "         (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))\n",
    "      \n",
    "      # torch.save(network.state_dict(), '/results/model.pth')\n",
    "      # torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in valloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(valloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valloader.dataset),\n",
    "    100. * correct / len(valloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "/home/cyy/.local/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.2691, Accuracy: 9285/10000 (93%)\n",
      "\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/GP_on_MNIST/DNN_mnist.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m test()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m   train(epoch)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000011vscode-remote?line=3'>4</a>\u001b[0m   test()\n",
      "\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/GP_on_MNIST/DNN_mnist.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000009vscode-remote?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000009vscode-remote?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000009vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(nth_derivative(output, data, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000009vscode-remote?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m log_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/GP_on_MNIST/DNN_mnist.ipynb Cell 10'\u001b[0m in \u001b[0;36mnth_derivative\u001b[0;34m(f, wrt, n)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000020vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnth_derivative\u001b[39m(f, wrt, n):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000020vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000020vscode-remote?line=6'>7</a>\u001b[0m         grads \u001b[39m=\u001b[39m grad(f, wrt, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000020vscode-remote?line=7'>8</a>\u001b[0m         f \u001b[39m=\u001b[39m grads\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000020vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m grads\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    255\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(outputs))\n\u001b[0;32m--> 260\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(loader):\n",
    "    network.eval()\n",
    "    # outputs = [] \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = network(data)\n",
    "            # outputs.append(output)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loader.dataset),\n",
    "        100. * correct / len(loader.dataset)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=False)\n",
    "tsloader = torch.utils.data.DataLoader(valset, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_964/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1178, Accuracy: 57779/60000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1224, Accuracy: 9613/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_score = scores(trloader)\n",
    "test_score = scores(tsloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/11191/Desktop/2022 Summer/Out of distribution learning/code/GP_on_MNIST/DNN_mnist.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39msave(train_score, \u001b[39m'\u001b[39m\u001b[39mtrain_score.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/11191/Desktop/2022%20Summer/Out%20of%20distribution%20learning/code/GP_on_MNIST/DNN_mnist.ipynb#ch0000016vscode-remote?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39msave(test_score, \u001b[39m'\u001b[39m\u001b[39mtest_score.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_score' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(train_score, 'train_score.pt')\n",
    "torch.save(test_score, 'test_score.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score = torch.load('train_score.pt')\n",
    "train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr[0:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_tr[0:500], train_score[0:500, 0])\n",
    "plt.show()\n",
    "# torch.scatter(train_score[0:200, 0], index = torch.from_numpy(y_tr[0:200]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
