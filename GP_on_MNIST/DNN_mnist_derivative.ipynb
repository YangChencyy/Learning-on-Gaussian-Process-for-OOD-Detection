{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from tensorflow import keras\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        self.image = torch.tensor(image, dtype = torch.float32)\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self): return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_ = self.image[idx]\n",
    "        image_ = image_[None, :]\n",
    "        label_ = self.label[idx]\n",
    "        return image_, label_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_ts, y_ts) = keras.datasets.mnist.load_data()\n",
    "x_tr, y_tr = x_tr[0:6400], y_tr[0:6400]\n",
    "x_ts, y_ts = x_ts[0:640], y_ts[0:640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = MNISTDataset(x_tr, y_tr)\n",
    "valset = MNISTDataset(x_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(trainloader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import grad\n",
    "\n",
    "# def nth_derivative(f, wrt, n):\n",
    "\n",
    "#     for i in range(n):\n",
    "\n",
    "#         grads = grad(f, wrt, create_graph=True)[0]\n",
    "#         f = grads.sum()\n",
    "\n",
    "#     return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative = [[]]*10\n",
    "# new_derivative = []*10\n",
    "\n",
    "def train(epoch):\n",
    "  \n",
    "  i = 0\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(trainloader):\n",
    "    data.requires_grad_(True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    # print(output.shape)\n",
    "    \n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    first_derivative = data.grad\n",
    "    # print(first_derivative.shape) # 64*1*28*28\n",
    "    first_derivative = first_derivative.view(64, -1)\n",
    "    # print(first_derivative[1].shape) # 64, 784\n",
    "    \n",
    "    # print(i)  \n",
    "    for j in range(len(first_derivative)):\n",
    "        # print(len(first_derivative))\n",
    "        derivative[y_tr[i]].append(first_derivative[j])\n",
    "        i += 1\n",
    "    # print(i)  \n",
    "    \n",
    "      \n",
    "    loss_all = []\n",
    "    for j in range(10):\n",
    "        temp = torch.stack(derivative[j])\n",
    "        # print(temp.shape)\n",
    "        mean = torch.mean(temp, dim = 0)\n",
    "        # print(mean.shape)\n",
    "        # print(derivative[0][0].shape)\n",
    "        l = 0\n",
    "        for k in range(len(derivative[j])):\n",
    "            loss2 = torch.nn.MSELoss()\n",
    "            l += loss2(mean, derivative[j][k])\n",
    "        loss_all.append(l)\n",
    "    \n",
    "    # print(loss_all)\n",
    "    derivative_loss = 10*sum(loss_all)\n",
    "    loss += derivative_loss\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print(\"derivative loss:\", derivative_loss)\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "        100. * batch_idx / len(trainloader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "         (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))\n",
    "    \n",
    "    # derivative = new_derivative\n",
    "    # new_derivative = []*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in valloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(valloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valloader.dataset),\n",
    "    100. * correct / len(valloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_790/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 15.7158, Accuracy: 61/640 (10%)\n",
      "\n",
      "derivative loss: tensor(3.3339e-05)\n",
      "Train Epoch: 1 [0/6400 (0%)]\tLoss: 21.824171\n",
      "derivative loss: tensor(0.0012)\n",
      "Train Epoch: 1 [320/6400 (5%)]\tLoss: 2.293024\n",
      "derivative loss: tensor(0.0019)\n",
      "Train Epoch: 1 [640/6400 (10%)]\tLoss: 2.307087\n",
      "derivative loss: tensor(0.0029)\n",
      "Train Epoch: 1 [960/6400 (15%)]\tLoss: 2.220138\n",
      "derivative loss: tensor(0.0039)\n",
      "Train Epoch: 1 [1280/6400 (20%)]\tLoss: 2.237751\n",
      "derivative loss: tensor(0.0050)\n",
      "Train Epoch: 1 [1600/6400 (25%)]\tLoss: 2.133295\n",
      "derivative loss: tensor(0.0060)\n",
      "Train Epoch: 1 [1920/6400 (30%)]\tLoss: 1.990435\n",
      "derivative loss: tensor(0.0067)\n",
      "Train Epoch: 1 [2240/6400 (35%)]\tLoss: 1.894405\n",
      "derivative loss: tensor(0.0075)\n",
      "Train Epoch: 1 [2560/6400 (40%)]\tLoss: 1.819620\n",
      "derivative loss: tensor(0.0082)\n",
      "Train Epoch: 1 [2880/6400 (45%)]\tLoss: 1.787635\n",
      "derivative loss: tensor(0.0090)\n",
      "Train Epoch: 1 [3200/6400 (50%)]\tLoss: 1.546015\n",
      "derivative loss: tensor(0.0100)\n",
      "Train Epoch: 1 [3520/6400 (55%)]\tLoss: 1.634689\n",
      "derivative loss: tensor(0.0106)\n",
      "Train Epoch: 1 [3840/6400 (60%)]\tLoss: 1.375104\n",
      "derivative loss: tensor(0.0116)\n",
      "Train Epoch: 1 [4160/6400 (65%)]\tLoss: 1.356269\n",
      "derivative loss: tensor(0.0124)\n",
      "Train Epoch: 1 [4480/6400 (70%)]\tLoss: 1.566881\n",
      "derivative loss: tensor(0.0128)\n",
      "Train Epoch: 1 [4800/6400 (75%)]\tLoss: 1.398018\n",
      "derivative loss: tensor(0.0134)\n",
      "Train Epoch: 1 [5120/6400 (80%)]\tLoss: 1.708542\n",
      "derivative loss: tensor(0.0140)\n",
      "Train Epoch: 1 [5440/6400 (85%)]\tLoss: 1.370117\n",
      "derivative loss: tensor(0.0146)\n",
      "Train Epoch: 1 [5760/6400 (90%)]\tLoss: 1.425263\n",
      "derivative loss: tensor(0.0153)\n",
      "Train Epoch: 1 [6080/6400 (95%)]\tLoss: 1.148568\n",
      "\n",
      "Test set: Avg. loss: 0.8401, Accuracy: 491/640 (77%)\n",
      "\n",
      "derivative loss: tensor(0.0158)\n",
      "Train Epoch: 2 [0/6400 (0%)]\tLoss: 1.348220\n",
      "derivative loss: tensor(0.0161)\n",
      "Train Epoch: 2 [320/6400 (5%)]\tLoss: 1.195510\n",
      "derivative loss: tensor(0.0166)\n",
      "Train Epoch: 2 [640/6400 (10%)]\tLoss: 1.070712\n",
      "derivative loss: tensor(0.0173)\n",
      "Train Epoch: 2 [960/6400 (15%)]\tLoss: 1.230520\n",
      "derivative loss: tensor(0.0178)\n",
      "Train Epoch: 2 [1280/6400 (20%)]\tLoss: 1.135500\n",
      "derivative loss: tensor(0.0182)\n",
      "Train Epoch: 2 [1600/6400 (25%)]\tLoss: 0.924617\n",
      "derivative loss: tensor(0.0186)\n",
      "Train Epoch: 2 [1920/6400 (30%)]\tLoss: 1.121907\n",
      "derivative loss: tensor(0.0192)\n",
      "Train Epoch: 2 [2240/6400 (35%)]\tLoss: 1.101778\n",
      "derivative loss: tensor(0.0196)\n",
      "Train Epoch: 2 [2560/6400 (40%)]\tLoss: 0.810824\n",
      "derivative loss: tensor(0.0202)\n",
      "Train Epoch: 2 [2880/6400 (45%)]\tLoss: 0.992268\n",
      "derivative loss: tensor(0.0208)\n",
      "Train Epoch: 2 [3200/6400 (50%)]\tLoss: 0.935089\n",
      "derivative loss: tensor(0.0214)\n",
      "Train Epoch: 2 [3520/6400 (55%)]\tLoss: 1.285826\n",
      "derivative loss: tensor(0.0222)\n",
      "Train Epoch: 2 [3840/6400 (60%)]\tLoss: 0.778364\n",
      "derivative loss: tensor(0.0228)\n",
      "Train Epoch: 2 [4160/6400 (65%)]\tLoss: 1.178713\n",
      "derivative loss: tensor(0.0234)\n",
      "Train Epoch: 2 [4480/6400 (70%)]\tLoss: 1.057910\n",
      "derivative loss: tensor(0.0239)\n",
      "Train Epoch: 2 [4800/6400 (75%)]\tLoss: 1.019518\n",
      "derivative loss: tensor(0.0245)\n",
      "Train Epoch: 2 [5120/6400 (80%)]\tLoss: 1.363104\n",
      "derivative loss: tensor(0.0250)\n",
      "Train Epoch: 2 [5440/6400 (85%)]\tLoss: 0.622614\n",
      "derivative loss: tensor(0.0256)\n",
      "Train Epoch: 2 [5760/6400 (90%)]\tLoss: 1.274767\n",
      "derivative loss: tensor(0.0262)\n",
      "Train Epoch: 2 [6080/6400 (95%)]\tLoss: 1.117323\n",
      "\n",
      "Test set: Avg. loss: 0.5948, Accuracy: 534/640 (83%)\n",
      "\n",
      "derivative loss: tensor(0.0267)\n",
      "Train Epoch: 3 [0/6400 (0%)]\tLoss: 0.922570\n",
      "derivative loss: tensor(0.0272)\n",
      "Train Epoch: 3 [320/6400 (5%)]\tLoss: 0.696077\n",
      "derivative loss: tensor(0.0278)\n",
      "Train Epoch: 3 [640/6400 (10%)]\tLoss: 0.912916\n",
      "derivative loss: tensor(0.0283)\n",
      "Train Epoch: 3 [960/6400 (15%)]\tLoss: 1.028677\n",
      "derivative loss: tensor(0.0290)\n",
      "Train Epoch: 3 [1280/6400 (20%)]\tLoss: 0.977194\n",
      "derivative loss: tensor(0.0295)\n",
      "Train Epoch: 3 [1600/6400 (25%)]\tLoss: 0.951189\n",
      "derivative loss: tensor(0.0300)\n",
      "Train Epoch: 3 [1920/6400 (30%)]\tLoss: 0.962046\n",
      "derivative loss: tensor(0.0305)\n",
      "Train Epoch: 3 [2240/6400 (35%)]\tLoss: 0.822622\n",
      "derivative loss: tensor(0.0311)\n",
      "Train Epoch: 3 [2560/6400 (40%)]\tLoss: 0.834875\n",
      "derivative loss: tensor(0.0318)\n",
      "Train Epoch: 3 [2880/6400 (45%)]\tLoss: 0.987455\n",
      "derivative loss: tensor(0.0324)\n",
      "Train Epoch: 3 [3200/6400 (50%)]\tLoss: 1.170393\n",
      "derivative loss: tensor(0.0331)\n",
      "Train Epoch: 3 [3520/6400 (55%)]\tLoss: 0.909967\n",
      "derivative loss: tensor(0.0339)\n",
      "Train Epoch: 3 [3840/6400 (60%)]\tLoss: 0.797381\n",
      "derivative loss: tensor(0.0345)\n",
      "Train Epoch: 3 [4160/6400 (65%)]\tLoss: 0.875004\n",
      "derivative loss: tensor(0.0352)\n",
      "Train Epoch: 3 [4480/6400 (70%)]\tLoss: 0.858633\n",
      "derivative loss: tensor(0.0356)\n",
      "Train Epoch: 3 [4800/6400 (75%)]\tLoss: 1.079726\n",
      "derivative loss: tensor(0.0361)\n",
      "Train Epoch: 3 [5120/6400 (80%)]\tLoss: 1.134259\n",
      "derivative loss: tensor(0.0365)\n",
      "Train Epoch: 3 [5440/6400 (85%)]\tLoss: 0.800765\n",
      "derivative loss: tensor(0.0373)\n",
      "Train Epoch: 3 [5760/6400 (90%)]\tLoss: 0.914052\n",
      "derivative loss: tensor(0.0377)\n",
      "Train Epoch: 3 [6080/6400 (95%)]\tLoss: 0.838452\n",
      "\n",
      "Test set: Avg. loss: 0.4823, Accuracy: 561/640 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(loader):\n",
    "    network.eval()\n",
    "    # outputs = [] \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = network(data)\n",
    "            # outputs.append(output)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loader.dataset),\n",
    "        100. * correct / len(loader.dataset)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "trloader = torch.utils.data.DataLoader(trainset, batch_size=6400, shuffle=False)\n",
    "tsloader = torch.utils.data.DataLoader(valset, batch_size=640, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_790/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.4051, Accuracy: 5745/6400 (90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4823, Accuracy: 561/640 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_score = scores(trloader)\n",
    "test_score = scores(tsloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6400, 10])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_score, 'train_score_1_1.pt')\n",
    "torch.save(test_score, 'test_score_1_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score = torch.load('train_score.pt')\n",
    "train_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr[0:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_tr[0:500], train_score[0:500, 0])\n",
    "plt.show()\n",
    "# torch.scatter(train_score[0:200, 0], index = torch.from_numpy(y_tr[0:200]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
