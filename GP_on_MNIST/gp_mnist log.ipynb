{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading UMAP data\n",
    "1. loading data from MNIST dataset\n",
    "2. choosing 600 data points from each training dataset (6000 in total)\n",
    "3. choosing data points in propotion in the testing sets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features_UMAP.npy', 'rb') as f:\n",
    "    train_x = np.load(f)\n",
    "    train_y_label = np.load(f)\n",
    "    test_x = np.load(f)\n",
    "    test_y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y_label, test_x, test_y = \\\n",
    "    torch.from_numpy(train_x), torch.from_numpy(train_y_label), \\\n",
    "        torch.from_numpy(test_x), torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training labels\n",
    "We use the scores of training samples from DNN to be the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6400, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = torch.load('train_score_1_1.pt')\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=torch.uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2000\n",
    "train_x, train_y = train_x[0: n], train_y[0: n, :]\n",
    "test_y = test_y.to(torch.int64)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1162, 2.9499, 2.8939,  ..., 3.0247, 3.1743, 3.0649],\n",
       "        [3.3749, 3.1004, 2.8967,  ..., 2.7033, 3.0462, 2.8206],\n",
       "        [2.9604, 2.8974, 3.0302,  ..., 3.0819, 3.0163, 3.2335],\n",
       "        ...,\n",
       "        [3.1766, 3.0519, 2.9361,  ..., 3.0577, 3.2233, 3.1238],\n",
       "        [3.1539, 3.3090, 3.3609,  ..., 3.2874, 3.2573, 3.1866],\n",
       "        [3.3748, 3.1160, 3.0213,  ..., 2.8930, 3.1180, 2.9055]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add = - torch.min(train_y) + 1\n",
    "log_y = add + train_y\n",
    "log_y = torch.log(log_y)\n",
    "log_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, likelihoods, mlls = [], [], []\n",
    "for j in range(10):\n",
    "    y = log_y[:, j]\n",
    "    x = train_x\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    likelihood.train()\n",
    "    model.train()\n",
    "    likelihoods.append(likelihood)\n",
    "    models.append(model)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    mlls.append(mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, likelihoods, mlls = [], [], []\n",
    "for j in range(10):\n",
    "    y = log_y[:, j]\n",
    "    x = train_x\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    likelihood.train()\n",
    "    model.train()\n",
    "    likelihoods.append(likelihood)\n",
    "    models.append(model)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    mlls.append(mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 10\n",
    "\n",
    "# Use the adam optimizer\n",
    "lr = 0.1\n",
    "opts = []\n",
    "for j in range(10):\n",
    "    optimizer = torch.optim.Adam(models[j].parameters(), lr = lr)  # Includes GaussianLikelihood parameters\n",
    "    opts.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Iter 1/10 - Loss: 0.943   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.881   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.827   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.771   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.728   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.676   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.627   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.584   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.536   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.493   lengthscale: 0.100   noise: 0.340\n",
      "Model 1\n",
      "Iter 1/10 - Loss: 0.943   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.883   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.826   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.776   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.720   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.673   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.628   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.582   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.534   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.487   lengthscale: 0.100   noise: 0.340\n",
      "Model 2\n",
      "Iter 1/10 - Loss: 0.947   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.887   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.836   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.782   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.732   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.689   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.640   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.599   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.547   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.507   lengthscale: 0.100   noise: 0.340\n",
      "Model 3\n",
      "Iter 1/10 - Loss: 0.966   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.906   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.853   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.805   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.759   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.712   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.671   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.629   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.583   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.543   lengthscale: 0.100   noise: 0.340\n",
      "Model 4\n",
      "Iter 1/10 - Loss: 0.945   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.884   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.826   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.775   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.723   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.673   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.623   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.583   lengthscale: 0.100   noise: 0.402\n",
      "Iter 9/10 - Loss: 0.534   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.492   lengthscale: 0.100   noise: 0.340\n",
      "Model 5\n",
      "Iter 1/10 - Loss: 0.942   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.889   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.833   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.772   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.726   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.680   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.630   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.581   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.543   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.492   lengthscale: 0.100   noise: 0.340\n",
      "Model 6\n",
      "Iter 1/10 - Loss: 0.945   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.891   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.834   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.781   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.734   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.687   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.638   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.594   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.552   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.505   lengthscale: 0.100   noise: 0.340\n",
      "Model 7\n",
      "Iter 1/10 - Loss: 0.947   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.893   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.830   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.784   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.731   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.682   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.638   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.593   lengthscale: 0.100   noise: 0.402\n",
      "Iter 9/10 - Loss: 0.546   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.499   lengthscale: 0.100   noise: 0.340\n",
      "Model 8\n",
      "Iter 1/10 - Loss: 0.941   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.885   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.823   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.774   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.723   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.674   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.626   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.577   lengthscale: 0.100   noise: 0.402\n",
      "Iter 9/10 - Loss: 0.529   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.481   lengthscale: 0.100   noise: 0.340\n",
      "Model 9\n",
      "Iter 1/10 - Loss: 0.946   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 0.883   lengthscale: 0.100   noise: 0.644\n",
      "Iter 3/10 - Loss: 0.827   lengthscale: 0.100   noise: 0.598\n",
      "Iter 4/10 - Loss: 0.775   lengthscale: 0.100   noise: 0.554\n",
      "Iter 5/10 - Loss: 0.730   lengthscale: 0.100   noise: 0.513\n",
      "Iter 6/10 - Loss: 0.679   lengthscale: 0.100   noise: 0.474\n",
      "Iter 7/10 - Loss: 0.628   lengthscale: 0.100   noise: 0.437\n",
      "Iter 8/10 - Loss: 0.591   lengthscale: 0.100   noise: 0.403\n",
      "Iter 9/10 - Loss: 0.542   lengthscale: 0.100   noise: 0.370\n",
      "Iter 10/10 - Loss: 0.492   lengthscale: 0.100   noise: 0.340\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(\"Model\", j)\n",
    "    model = models[j]\n",
    "    likelihood = likelihoods[j]\n",
    "    mll = mlls[j]\n",
    "    optimizer = opts[j]\n",
    "    y = log_y[:, j]\n",
    "    for i in range(training_iter):                \n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # print(output)\n",
    "        \n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, y)\n",
    "        loss.backward()\n",
    "          \n",
    "        if i % 1 == 0:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                #model.covar_module.base_kernel.lengthscale.item(),\n",
    "                0.1,\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "for model in models:\n",
    "    model.eval()\n",
    "for likelihood in likelihoods:\n",
    "    likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for j in range(10):\n",
    "    with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "        test_dist = models[j](test_x)\n",
    "        pred_means = test_dist.loc # F.log_softmax(test_dist.loc, dim = 0)\n",
    "        # mean = torch.mean(pred_means)\n",
    "        # pred_means = pred_means - mean\n",
    "        # pred_means = pred_means/torch.max(pred_means)\n",
    "        pred_means = torch.exp(pred_means) - add\n",
    "        scores.append(pred_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.stack(scores)\n",
    "# scores = F.log_softmax(scores, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16.2327,  -0.5029,  -6.2313,  -2.1530,  -7.8336,  -6.2632, -14.4673],\n",
       "        [-16.1729,  -3.5905,  -3.3923,  -5.4321,  -7.7878,  -3.5841, -13.7970],\n",
       "        [-14.3248,  -3.5387,  -5.0591,  -5.5451,  -6.6715,  -4.6552, -14.0583],\n",
       "        [-13.2891,  -2.1100,  -7.7058,  -4.2057,  -8.0328,  -7.2913, -14.1402],\n",
       "        [-15.1589,  -4.4668,  -6.4279,  -4.1468,  -2.6314,  -6.3724, -13.1631],\n",
       "        [-14.4368,  -0.6987,  -8.3945,  -4.4947,  -5.6316,  -8.2445, -14.1152],\n",
       "        [-17.1779,  -4.4238,  -7.9163,  -2.8529,  -6.4350,  -8.0278, -14.4344],\n",
       "        [-12.0922,  -3.3691,  -5.2750,  -4.2974,  -3.6169,  -4.9214, -12.7251],\n",
       "        [-14.0580,  -2.5960,  -5.2324,  -3.9002,  -5.0830,  -5.0630, -13.5020],\n",
       "        [-13.8602,  -2.9103,  -8.0139,  -5.2001,  -1.9612,  -7.8817, -12.9287]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:, 0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.422"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# scores = torch.stack(scores)\n",
    "pred_y = torch.argmax(scores, dim = 0)\n",
    "accuracy_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict score shape: torch.Size([10, 1000])\n",
      "real score shape: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "test_score = torch.load('test_score.pt')\n",
    "test_score = torch.transpose(test_score[0:1000, :], 0, 1)\n",
    "print(\"predict score shape:\", scores.shape)\n",
    "print(\"real score shape:\", test_score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 0, 1, 0, 9, 1, 7, 4, 9, 7, 6, 5, 7, 6, 0, 3, 9, 3, 3, 9, 7, 5, 5, 3,\n",
       "        4, 0, 5, 9, 0, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 loss for model 0 : tensor(93.0665)   mean: tensor(-7.8625)\n",
      "L2 loss for model 1 : tensor(76.5207)   mean: tensor(-8.2426)\n",
      "L2 loss for model 2 : tensor(102.1648)   mean: tensor(-8.0128)\n",
      "L2 loss for model 3 : tensor(102.3191)   mean: tensor(-6.4702)\n",
      "L2 loss for model 4 : tensor(95.2256)   mean: tensor(-7.7280)\n",
      "L2 loss for model 5 : tensor(82.3007)   mean: tensor(-7.0859)\n",
      "L2 loss for model 6 : tensor(106.1899)   mean: tensor(-9.2806)\n",
      "L2 loss for model 7 : tensor(132.0182)   mean: tensor(-7.1732)\n",
      "L2 loss for model 8 : tensor(42.3574)   mean: tensor(-6.9393)\n",
      "L2 loss for model 9 : tensor(73.7541)   mean: tensor(-8.2400)\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    output = loss(scores[j, :], test_score[j, :])\n",
    "    print(\"L2 loss for model\", j, \":\", output, \"  mean:\", torch.mean(scores[j, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
