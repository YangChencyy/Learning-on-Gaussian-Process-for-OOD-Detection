{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyy/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading UMAP data\n",
    "1. loading data from MNIST dataset\n",
    "2. choosing 600 data points from each training dataset (6000 in total)\n",
    "3. choosing data points in propotion in the testing sets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features_UMAP.npy', 'rb') as f:\n",
    "    train_x = np.load(f)\n",
    "    train_y_label = np.load(f)\n",
    "    test_x = np.load(f)\n",
    "    test_y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y_label, test_x, test_y = \\\n",
    "    torch.from_numpy(train_x), torch.from_numpy(train_y_label), \\\n",
    "        torch.from_numpy(test_x), torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training labels\n",
    "We use the scores of training samples from DNN to be the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = torch.load('train_score.pt')\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4306e+01, -1.5714e+01, -2.2822e+01, -5.8237e+00, -1.7193e+01,\n",
       "         -3.1217e-03, -1.4866e+01, -1.8692e+01, -9.3796e+00, -9.5028e+00],\n",
       "        [-4.7684e-07, -2.6279e+01, -1.7861e+01, -2.0814e+01, -2.3834e+01,\n",
       "         -1.9055e+01, -1.5301e+01, -1.9174e+01, -1.6309e+01, -1.5895e+01],\n",
       "        [-8.9569e+00, -8.6896e+00, -5.6311e+00, -9.2064e+00, -7.1852e-03,\n",
       "         -1.1038e+01, -7.7042e+00, -7.2663e+00, -8.8430e+00, -6.2833e+00],\n",
       "        [-1.6103e+01, -4.4345e-05, -1.2211e+01, -1.6662e+01, -1.1207e+01,\n",
       "         -1.4991e+01, -1.2636e+01, -1.3914e+01, -1.0783e+01, -1.4988e+01],\n",
       "        [-1.5714e+01, -1.5829e+01, -1.6959e+01, -1.6687e+01, -5.6171e+00,\n",
       "         -1.2577e+01, -1.8971e+01, -1.1049e+01, -8.2875e+00, -3.9143e-03],\n",
       "        [-1.5772e+01, -1.6557e+01, -8.4638e-06, -1.4882e+01, -2.0779e+01,\n",
       "         -2.5425e+01, -2.2365e+01, -1.2368e+01, -1.2529e+01, -2.1784e+01],\n",
       "        [-2.1715e+01, -7.1526e-07, -1.7992e+01, -2.3148e+01, -1.5851e+01,\n",
       "         -1.9909e+01, -1.6972e+01, -2.0787e+01, -1.4369e+01, -2.0067e+01]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0:7, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=torch.uint8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2000\n",
    "train_x, train_y = train_x[0: n], train_y[0: n, :]\n",
    "test_y = test_y.to(torch.int64)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, likelihoods, mlls = [], [], []\n",
    "for j in range(10):\n",
    "    y = train_y[:, j]\n",
    "    x = train_x\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    likelihood.train()\n",
    "    model.train()\n",
    "    likelihoods.append(likelihood)\n",
    "    models.append(model)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    mlls.append(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_index = np.arange(1, n, 10)\n",
    "tr_y = train_y[val_index, 1]\n",
    "tr_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing Cross Validation on Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, likelihoods, mlls = [], [], []\n",
    "for j in range(10):\n",
    "    y = train_y[:, j]\n",
    "    x = train_x\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    likelihood.train()\n",
    "    model.train()\n",
    "    likelihoods.append(likelihood)\n",
    "    models.append(model)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    mlls.append(mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 10\n",
    "\n",
    "# Use the adam optimizer\n",
    "lr = 0.1\n",
    "opts = []\n",
    "for j in range(10):\n",
    "    optimizer = torch.optim.Adam(models[j].parameters(), lr = lr)  # Includes GaussianLikelihood parameters\n",
    "    opts.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Iter 1/10 - Loss: 15.020   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 14.055   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 13.183   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 12.390   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 11.680   lengthscale: 0.100   noise: 0.911\n",
      "Iter 6/10 - Loss: 11.040   lengthscale: 0.100   noise: 0.970\n",
      "Iter 7/10 - Loss: 10.460   lengthscale: 0.100   noise: 1.031\n",
      "Iter 8/10 - Loss: 9.933   lengthscale: 0.100   noise: 1.093\n",
      "Iter 9/10 - Loss: 9.468   lengthscale: 0.100   noise: 1.155\n",
      "Iter 10/10 - Loss: 9.044   lengthscale: 0.100   noise: 1.218\n",
      "Model 1\n",
      "Iter 1/10 - Loss: 15.414   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 14.357   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 13.421   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 12.602   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 11.876   lengthscale: 0.100   noise: 0.912\n",
      "Iter 6/10 - Loss: 11.232   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 10.654   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 10.134   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 9.663   lengthscale: 0.100   noise: 1.158\n",
      "Iter 10/10 - Loss: 9.236   lengthscale: 0.100   noise: 1.222\n",
      "Model 2\n",
      "Iter 1/10 - Loss: 18.059   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 16.861   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 15.797   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 14.848   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 13.991   lengthscale: 0.100   noise: 0.912\n",
      "Iter 6/10 - Loss: 13.203   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 12.501   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 11.860   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 11.282   lengthscale: 0.100   noise: 1.157\n",
      "Iter 10/10 - Loss: 10.766   lengthscale: 0.100   noise: 1.221\n",
      "Model 3\n",
      "Iter 1/10 - Loss: 15.146   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 14.124   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 13.225   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 12.427   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 11.723   lengthscale: 0.100   noise: 0.912\n",
      "Iter 6/10 - Loss: 11.088   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 10.521   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 9.997   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 9.532   lengthscale: 0.100   noise: 1.157\n",
      "Iter 10/10 - Loss: 9.106   lengthscale: 0.100   noise: 1.221\n",
      "Model 4\n",
      "Iter 1/10 - Loss: 13.600   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 12.660   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 11.837   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 11.125   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 10.487   lengthscale: 0.100   noise: 0.912\n",
      "Iter 6/10 - Loss: 9.926   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 9.422   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 8.974   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 8.563   lengthscale: 0.100   noise: 1.158\n",
      "Iter 10/10 - Loss: 8.196   lengthscale: 0.100   noise: 1.222\n",
      "Model 5\n",
      "Iter 1/10 - Loss: 14.417   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 13.448   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 12.593   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 11.839   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 11.169   lengthscale: 0.100   noise: 0.911\n",
      "Iter 6/10 - Loss: 10.573   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 10.033   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 9.543   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 9.097   lengthscale: 0.100   noise: 1.157\n",
      "Iter 10/10 - Loss: 8.701   lengthscale: 0.100   noise: 1.221\n",
      "Model 6\n",
      "Iter 1/10 - Loss: 21.585   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 20.164   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 18.875   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 17.716   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 16.657   lengthscale: 0.100   noise: 0.911\n",
      "Iter 6/10 - Loss: 15.712   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 14.852   lengthscale: 0.100   noise: 1.031\n",
      "Iter 8/10 - Loss: 14.087   lengthscale: 0.100   noise: 1.093\n",
      "Iter 9/10 - Loss: 13.386   lengthscale: 0.100   noise: 1.156\n",
      "Iter 10/10 - Loss: 12.751   lengthscale: 0.100   noise: 1.219\n",
      "Model 7\n",
      "Iter 1/10 - Loss: 17.753   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 16.547   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 15.489   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 14.550   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 13.708   lengthscale: 0.100   noise: 0.912\n",
      "Iter 6/10 - Loss: 12.953   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 12.269   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 11.653   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 11.092   lengthscale: 0.100   noise: 1.158\n",
      "Iter 10/10 - Loss: 10.579   lengthscale: 0.100   noise: 1.222\n",
      "Model 8\n",
      "Iter 1/10 - Loss: 9.374   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 8.783   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 8.263   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 7.803   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 7.393   lengthscale: 0.100   noise: 0.911\n",
      "Iter 6/10 - Loss: 7.024   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 6.688   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 6.385   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 6.108   lengthscale: 0.100   noise: 1.157\n",
      "Iter 10/10 - Loss: 5.861   lengthscale: 0.100   noise: 1.221\n",
      "Model 9\n",
      "Iter 1/10 - Loss: 12.394   lengthscale: 0.100   noise: 0.693\n",
      "Iter 2/10 - Loss: 11.570   lengthscale: 0.100   noise: 0.744\n",
      "Iter 3/10 - Loss: 10.836   lengthscale: 0.100   noise: 0.798\n",
      "Iter 4/10 - Loss: 10.197   lengthscale: 0.100   noise: 0.854\n",
      "Iter 5/10 - Loss: 9.632   lengthscale: 0.100   noise: 0.911\n",
      "Iter 6/10 - Loss: 9.131   lengthscale: 0.100   noise: 0.971\n",
      "Iter 7/10 - Loss: 8.673   lengthscale: 0.100   noise: 1.032\n",
      "Iter 8/10 - Loss: 8.265   lengthscale: 0.100   noise: 1.094\n",
      "Iter 9/10 - Loss: 7.893   lengthscale: 0.100   noise: 1.157\n",
      "Iter 10/10 - Loss: 7.554   lengthscale: 0.100   noise: 1.221\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    print(\"Model\", j)\n",
    "    model = models[j]\n",
    "    likelihood = likelihoods[j]\n",
    "    mll = mlls[j]\n",
    "    optimizer = opts[j]\n",
    "    y = train_y[:, j]\n",
    "    for i in range(training_iter):                \n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # print(output)\n",
    "        \n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, y)\n",
    "        loss.backward()\n",
    "          \n",
    "        if i % 1 == 0:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                #model.covar_module.base_kernel.lengthscale.item(),\n",
    "                0.1,\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "for model in models:\n",
    "    model.eval()\n",
    "for likelihood in likelihoods:\n",
    "    likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for j in range(10):\n",
    "    with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "        test_dist = models[j](test_x)\n",
    "        pred_means = test_dist.loc # F.log_softmax(test_dist.loc, dim = 0)\n",
    "        # mean = torch.mean(pred_means)\n",
    "        # pred_means = pred_means - mean\n",
    "        # pred_means = pred_means/torch.max(pred_means)\n",
    "        scores.append(pred_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.stack(scores)\n",
    "# scores = F.log_softmax(scores, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.8704,   0.5921, -10.8498,  -0.8784, -10.1020, -11.0436,  -1.9011],\n",
       "        [ -3.4424,  -6.4728,  -3.7613,  -8.5433, -13.4269,  -4.1157,  -5.6005],\n",
       "        [ -1.2385,   1.7077,  -3.4207,  -5.6487, -10.6595,  -2.6236,  -1.9821],\n",
       "        [ -0.9181,  -1.8034,  -7.8738,  -4.0877, -13.2874,  -7.5424,  -4.9014],\n",
       "        [ -3.2178,  -8.8100,  -9.6771,  -8.0231,  -3.1888,  -9.9022,  -1.0033],\n",
       "        [ -3.4878,  -7.9205, -14.0575,   1.6861, -12.2117, -14.3890,  -5.1612],\n",
       "        [ -3.1249,  -1.5690, -12.8174,  -0.4174, -11.2836, -13.8376,  -1.8786],\n",
       "        [ -0.0504,  -1.7671,  -6.1910,  -7.5221,  -8.6065,  -5.8387,  -3.4029],\n",
       "        [ -1.8469,  -0.8593,  -8.2958,  -2.2124,  -8.7294,  -8.2936,  -2.7910],\n",
       "        [ -0.4189,  -5.9738, -12.0617,  -4.3757,  -4.4351, -12.0565,  -2.9092]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:, 0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.415"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# scores = torch.stack(scores)\n",
    "pred_y = torch.argmax(scores, dim = 0)\n",
    "accuracy_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict score shape: torch.Size([10, 1000])\n",
      "real score shape: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "test_score = torch.load('test_score.pt')\n",
    "test_score = torch.transpose(test_score[0:1000, :], 0, 1)\n",
    "print(\"predict score shape:\", scores.shape)\n",
    "print(\"real score shape:\", test_score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 2, 5, 4, 2, 4, 4, 9, 7, 5, 5, 4, 5, 1, 3, 4, 3, 3, 4, 7, 5, 5, 3,\n",
       "        4, 5, 9, 4, 5, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 loss for model 0 : tensor(79.5508)   mean: tensor(-9.6709)\n",
      "L2 loss for model 1 : tensor(50.5827)   mean: tensor(-9.5833)\n",
      "L2 loss for model 2 : tensor(91.9003)   mean: tensor(-8.1926)\n",
      "L2 loss for model 3 : tensor(77.2568)   mean: tensor(-7.3310)\n",
      "L2 loss for model 4 : tensor(47.5791)   mean: tensor(-10.2698)\n",
      "L2 loss for model 5 : tensor(84.2119)   mean: tensor(-8.7220)\n",
      "L2 loss for model 6 : tensor(142.7697)   mean: tensor(-10.8157)\n",
      "L2 loss for model 7 : tensor(67.1875)   mean: tensor(-9.7484)\n",
      "L2 loss for model 8 : tensor(40.1538)   mean: tensor(-6.6988)\n",
      "L2 loss for model 9 : tensor(41.5981)   mean: tensor(-9.2816)\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    output = loss(scores[j, :], test_score[j, :])\n",
    "    print(\"L2 loss for model\", j, \":\", output, \"  mean:\", torch.mean(scores[j, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
